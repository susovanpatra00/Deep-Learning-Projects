{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ae7827-da36-4a56-8318-3015738383a2",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994c0da-0408-42f3-b598-b44b4b6a860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae16fc-49eb-4930-8dd5-f2996cf2222c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36c04bcc-c0aa-416d-8256-9ec42f7b522a",
   "metadata": {},
   "source": [
    "## Downloading the Rice Image Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b3a3c-4228-4953-8236-c2d225e065e2",
   "metadata": {},
   "source": [
    "Dataset Link - https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset/data\n",
    "\n",
    "The Dataset is available in Kaggle. \n",
    "\n",
    "\n",
    "1.   Steps for downloading the dataset : \n",
    "\n",
    "*   !pip install kaggle\n",
    "*   ! mkdir ~/.kaggle\n",
    "*   ! cp kaggle.json ~/.kaggle/\n",
    "*   ! chmod 600 ~/.kaggle/kaggle.json\n",
    "*   ! kaggle datasets download muratkokludataset/rice-image-dataset\n",
    "\n",
    "2.   Steps for unzipping the dataset :\n",
    "\n",
    "      import zipfile <br>\n",
    "      zip_file_path = '/content/rice-image-dataset.zip' <br>\n",
    "      with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:<br>\n",
    "      &emsp; zip_ref.extractall('/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ade218-f82e-4869-9dfd-ba329c136a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dadc6d66-8dd8-48b2-8378-f15f4ba19eac",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089188fb-d10f-4733-98d2-4ae643a368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255, validation_split=0.3)\n",
    "train_ds = train_datagen.flow_from_directory('Rice_Image_Dataset', subset = 'training', target_size=(256,256), batch_size = 32)\n",
    "val_ds = train_datagen.flow_from_directory('Rice_Image_Dataset', subset = 'validation', target_size=(256,256), batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366aea94-2fb2-45f1-85d4-f068a973df1d",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used in machine learning and computer vision, particularly in the context of training deep learning models, to artificially increase the diversity of the training dataset. The idea is to apply various random transformations to the existing training data, creating new variations of the input samples. This helps the model generalize better to unseen data and improves its robustness. In the context of image data, data augmentation typically involves applying random transformations to images. Common augmentations include:\n",
    "\n",
    "\n",
    "*   Rotation: Rotating the image by a random angle.\n",
    "*   Width and Height Shifts: Shifting the image horizontally and vertically by a fraction of its total width and height, respectively.\n",
    "*   Zooming: Zooming into or out of the image.\n",
    "*   Flipping: Flipping the image horizontally or vertically.\n",
    "*   Shearing: Shearing the image by a certain angle.\n",
    "*   Brightness and Contrast Adjustments: Changing the brightness and contrast of the image.\n",
    "\n",
    "\n",
    "By applying these random transformations, the model sees slightly different versions of the input data during each epoch of training. \n",
    "This helps the model become more invariant to variations in the input data and reduces overfitting. Data augmentation is particularly useful when the available training dataset is limited.\n",
    "\n",
    "In the context of deep learning frameworks like TensorFlow and Keras, the ImageDataGenerator class is often used to perform data\n",
    "augmentation. This class allows you to specify various augmentation parameters, and it generates augmented images on-the-fly during the \n",
    "training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb4698-a36e-4eb8-a719-f92d01824d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = train_ds.image_shape\n",
    "num_classes = len(train_ds.class_indices)\n",
    "\n",
    "print(\"Image shape in train dataset:\", train_ds.image_shape)\n",
    "print(\"Image shape in validation dataset:\", val_ds.image_shape)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8aaf37-50f2-4cbc-a1d0-c5594c04df06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92378df3-facc-4fe8-9760-87bbf5321e50",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d495ab4-d6cd-4e45-bbda-7da14e07f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_images, batch_labels = next(train_ds)\n",
    "num_images = 16\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n",
    "axes = axes.ravel()  \n",
    "\n",
    "num_classes = batch_labels.shape[-1]  \n",
    "rice_class_names = {0: 'Arborio', 1: 'Basmati', 2: 'Ipsala', 3: 'Jasmine', 4: 'Karacadag'}\n",
    "\n",
    "for i in range(num_images):\n",
    "    ax = axes[i]\n",
    "    img = batch_images[i]\n",
    "    one_hot_label = batch_labels[i]\n",
    "    class_idx = np.argmax(one_hot_label) \n",
    "    class_name = rice_class_names.get(class_idx, 'Unknown')  \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Rice Type: {class_name}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f119847-f7eb-44ca-9cc0-9a7d5a7fa1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a01795e-cbe4-4ea2-a664-a212fa9c0286",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254f90c-7bee-4c62-be4b-fa3dfda9f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(8, (4, 4), activation='relu', input_shape = image_shape, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((8,8), strides=(8,8)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(16, (4, 4), activation='relu', input_shape = image_shape, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D((8,8), strides=(8,8)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc954715-fff9-4c6d-86e7-7c7304e01943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d50de4-5701-42f6-a42f-f7f28140f41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d63beb2-1761-4855-964b-51fee6f46c60",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7032885-4160-4a29-8c80-fc141450520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180c8c3f-f498-4d07-a23a-0146a53d3936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd54edb-62d5-4bfb-98f2-875b261eb5b0",
   "metadata": {},
   "source": [
    "## Compile & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4c3a9-bee8-4589-9bb8-e8b6bc295039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'], )\n",
    "history = model.fit(train_ds, epochs = 10, validation_data = val_ds, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293aae3-f1ef-4ade-8c24-f88f5e019e06",
   "metadata": {},
   "source": [
    "We can see that the loss is around 2 and the accuracy is more than 98%,,,which is very good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fed094-50f0-462a-a58d-a4696194eb8e",
   "metadata": {},
   "source": [
    "For evaluating the model's performnace we can use Confusion Matrix, Classification Report, Visualize Training and Validation Accuracy/Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10b3c5-ba6d-4c03-b93e-c6d968996941",
   "metadata": {},
   "source": [
    "In multiclass classification problems, where there are more than two classes, there are two main strategies to transform the problem into multiple binary classification problems: One-vs-One (OvO) and One-vs-All (OvA), also known as One-vs-Rest (OvR).\n",
    "\n",
    "**One-vs-One (OvO)**:\n",
    "In the OvO approach, a separate binary classifier is trained for each pair of classes. If there are N classes, N(N-1)/2 binary classifiers are trained, each distinguishing between a pair of classes. During prediction, each binary classifier votes for one of the two classes, and the class with the most votes is selected as the final prediction.\n",
    "\n",
    "For example, if there are three classes (A, B, and C), three binary classifiers would be trained:\n",
    "1. A vs. B\n",
    "2. A vs. C\n",
    "3. B vs. C\n",
    "\n",
    "To classify a new instance, all three binary classifiers make predictions, and the class with the most votes is chosen as the final prediction.\n",
    "\n",
    "**One-vs-All (OvA) or One-vs-Rest (OvR)**:\n",
    "In the OvA approach, N binary classifiers are trained, one for each class. For each class, a binary classifier is trained to distinguish that class from all the other classes combined. During prediction, the classifier with the highest confidence score or probability is selected, and the corresponding class is the predicted class.\n",
    "\n",
    "For example, if there are three classes (A, B, and C), three binary classifiers would be trained:\n",
    "1. A vs. (B+C)\n",
    "2. B vs. (A+C)\n",
    "3. C vs. (A+B)\n",
    "\n",
    "To classify a new instance, all three binary classifiers make predictions, and the classifier with the highest confidence or probability determines the predicted class.\n",
    "\n",
    "The choice between OvO and OvA depends on the specific problem and the characteristics of the dataset, such as the number of classes, the class distribution, and the complexity of the decision boundaries. In general, OvO tends to perform better when there are many classes, while OvA can be more efficient for problems with fewer classes.\n",
    "\n",
    "But here as I used Deep Learning to solve the problem it is important to note that deep learning frameworks, such as TensorFlow and PyTorch, often handle multiclass classification problems directly, without explicitly implementing OvO or OvA strategies. Instead, they use techniques like softmax activation and cross-entropy loss to automatically handle multiple classes during training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea6591-34ae-4b25-a658-28823579f435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
